{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image as PImage\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from typing import List\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = './train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>র্টি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  grapheme_root  vowel_diacritic  consonant_diacritic grapheme\n",
       "0  Train_0             15                9                    5   ক্ট্রো\n",
       "1  Train_1            159                0                    0        হ\n",
       "2  Train_2             22                3                    5     খ্রী\n",
       "3  Train_3             53                2                    2     র্টি\n",
       "4  Train_4             71                9                    5     থ্রো"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_ = pd.read_csv('./train.csv')\n",
    "train_df_[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']] = train_df_[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].astype('uint8')\n",
    "train_df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    train_df = pd.merge(pd.read_parquet(os.path.join(DATA_FOLDER, 'train_image_data_{}.parquet'.format(i))),\n",
    "                        train_df_, on='image_id').drop(['image_id'], axis=1)\n",
    "#to image\n",
    "train_labels = train_df[['grapheme_root','vowel_diacritic','consonant_diacritic','grapheme']]\n",
    "train_df.drop(['grapheme_root','vowel_diacritic','consonant_diacritic','grapheme'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/anaconda/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/opt/anaconda/lib/python3.7/site-packages/pandas/core/series.py:1295: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._set_labels(key, value)\n",
      "/opt/anaconda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3326: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "train_labels['vowel_consonant_diacritic_pair'] = train_labels['vowel_diacritic'].astype(str) + '_' + train_labels['consonant_diacritic'].astype(str)\n",
    "train_labels['test_fold'] = 0 \n",
    "for idx, (train_index, test_index) in enumerate(skf.split(X = train_labels['vowel_consonant_diacritic_pair'], \n",
    "                                                          y = train_labels['vowel_consonant_diacritic_pair'], \n",
    "                                                          groups=train_labels['vowel_consonant_diacritic_pair'])):\n",
    "    train_labels['test_fold'][test_index] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Callbacks and Dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 df, \n",
    "                 labels,\n",
    "                 transforms=None):\n",
    "        \n",
    "        self.df = df\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        flattened_image = self.df.iloc[idx].values.astype(np.uint8)\n",
    "        image = np.expand_dims(flattened_image.reshape(137, 236), 2)\n",
    "        \n",
    "        grapheme_root =  self.labels['grapheme_root'].values[idx]\n",
    "        vowel_diacritic = self.labels['vowel_diacritic'].values[idx]\n",
    "        consonant_diacritic = self.labels['consonant_diacritic'].values[idx]\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            augmented = self.transforms(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        image = torch.from_numpy(image.transpose((2,0,1)))\n",
    "        grapheme_root = torch.tensor(grapheme_root).long()\n",
    "        vowel_diacritic = torch.tensor(vowel_diacritic).long()\n",
    "        consonant_diacritic = torch.tensor(consonant_diacritic).long() \n",
    "        \n",
    "        output_dict  = {\n",
    "            'grapheme_root' : grapheme_root, \n",
    "            'vowel_diacritic' : vowel_diacritic, \n",
    "            'consonant_diacritic' : consonant_diacritic, \n",
    "            'image' : image\n",
    "                       }\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "train_fold_idx = train_labels['test_fold'] != idx\n",
    "val_fold_idx = train_labels['test_fold'] == idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_val = A.Compose([\n",
    "    A.Normalize(mean=(0.485), std=(0.229))\n",
    "],\n",
    "        p=1.0)  \n",
    "train_dataset = ImageDataset(df = train_df.loc[train_fold_idx, :], \n",
    "                             labels = train_labels.loc[train_fold_idx, :], \n",
    "                             transforms = aug_val\n",
    "                            )\n",
    "val_dataset = ImageDataset(df = train_df.loc[val_fold_idx, :],\n",
    "                           labels = train_labels.loc[val_fold_idx, :], \n",
    "                           transforms = aug_val\n",
    "                          )\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=False   \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks for catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alchemy not available, to install alchemy, run `pip install alchemy-catalyst`.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "from catalyst.dl import Callback, RunnerState, MetricCallback, CallbackOrder, CriterionCallback\n",
    "\n",
    "class TaskMetricCallback(Callback):\n",
    "    '''\n",
    "    Proposed metrics:\n",
    "    import numpy as np\n",
    "    import sklearn.metrics\n",
    "\n",
    "    scores = []\n",
    "    for component in ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic']:\n",
    "        y_true_subset = solution[solution[component] == component]['target'].values\n",
    "        y_pred_subset = submission[submission[component] == component]['target'].values\n",
    "        scores.append(sklearn.metrics.recall_score(\n",
    "            y_true_subset, y_pred_subset, average='macro'))\n",
    "    final_score = np.average(scores, weights=[2,1,1])\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_key: str = ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic'], \n",
    "        output_key: str = ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic'],\n",
    "        class_names: str = ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic'],\n",
    "        prefix: str = \"taskmetric\", \n",
    "        ignore_index=None\n",
    "    ):\n",
    "        super().__init__(CallbackOrder.Metric)\n",
    "        self.metric_fn = lambda outputs, targets: recall_score(targets, outputs, average=\"macro\")\n",
    "        self.prefix = prefix\n",
    "        self.output_key = output_key\n",
    "        self.input_key = input_key\n",
    "        self.class_names = class_names\n",
    "        self.outputs = [[] for i in range(3)]\n",
    "        self.targets = [[] for i in range(3)]\n",
    "\n",
    "    def on_batch_end(self, state: RunnerState):\n",
    "        \n",
    "        for i in range(3):\n",
    "            outputs = state.output[self.output_key[i]].detach().cpu().numpy()\n",
    "            targets = state.input[self.input_key[i]].detach().cpu().numpy()\n",
    "            #num_classes = outputs.shape[1]\n",
    "            outputs = np.argmax(outputs, axis=1)\n",
    "            #outputs = [np.eye(num_classes)[y] for y in outputs]\n",
    "            #targets = [np.eye(num_classes)[y] for y in targets]\n",
    "            self.outputs[i].extend(outputs)\n",
    "            self.targets[i].extend(targets)\n",
    "\n",
    "    def on_loader_start(self, state):\n",
    "        self.outputs = [[] for i in range(3)]\n",
    "        self.targets = [[] for i in range(3)]\n",
    "\n",
    "    def on_loader_end(self, state):\n",
    "        metric_name = self.prefix\n",
    "        score_vec = []\n",
    "        for i in range(3):\n",
    "            targets = np.array(self.targets[i])\n",
    "            outputs = np.array(self.outputs[i])\n",
    "            metric = self.metric_fn(outputs, targets)\n",
    "            score_vec.append(metric)\n",
    "            state.metrics.epoch_values[state.loader_name][self.class_names[i]] = float(metric)\n",
    "            \n",
    "            \n",
    "        state.metrics.epoch_values[state.loader_name][metric_name] = np.average(score_vec, weights=[2,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,stride=1,kernel_size=3,padding=1,bias=False):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        self.cnn1 =nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding,bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels,out_channels,kernel_size,1,padding,bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride,bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Sequential()\n",
    "    def forward(self,x):\n",
    "        residual = x\n",
    "        x = self.cnn1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += self.shortcut(residual)\n",
    "        x = nn.ReLU(True)(x)\n",
    "        return x\n",
    "class ResNet18(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(ResNet18,self).__init__()\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1,64,kernel_size=2,stride=2,padding=3,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.MaxPool2d(1,1),\n",
    "            ResidualBlock(64,64),\n",
    "            ResidualBlock(64,64,2)\n",
    "        )\n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            ResidualBlock(64,128),\n",
    "            ResidualBlock(128,128,2)\n",
    "        )\n",
    "        self.block4 = nn.Sequential(\n",
    "            ResidualBlock(128,256),\n",
    "            ResidualBlock(256,256,2)\n",
    "        )\n",
    "        self.block5 = nn.Sequential(\n",
    "            ResidualBlock(256,512),\n",
    "            ResidualBlock(512,512,2)\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        # vowel_diacritic\n",
    "        self.fc1 = nn.Linear(512,11)\n",
    "        # grapheme_root\n",
    "        self.fc2 = nn.Linear(512,168)\n",
    "        # consonant_diacritic\n",
    "        self.fc3 = nn.Linear(512,7)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x1 = self.fc1(x)\n",
    "        x2 = self.fc2(x)\n",
    "        x3 = self.fc3(x)\n",
    "        return({'vowel_diacritic':x1,\n",
    "                'grapheme_root':x2,\n",
    "                'consonant_diacritic':x3})\n",
    "class ResNet34(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(ResNet34,self).__init__()\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1,64,kernel_size=2,stride=2,padding=3,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.MaxPool2d(1,1),\n",
    "            ResidualBlock(64,64),\n",
    "            ResidualBlock(64,64,2)\n",
    "        )\n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            ResidualBlock(64,128),\n",
    "            ResidualBlock(128,128,2)\n",
    "        )\n",
    "        \n",
    "        self.block4 = nn.Sequential(\n",
    "            ResidualBlock(128,256),\n",
    "            ResidualBlock(256,256,2)\n",
    "        )\n",
    "        self.block5 = nn.Sequential(\n",
    "            ResidualBlock(256,512),\n",
    "            ResidualBlock(512,512,2)\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        # vowel_diacritic\n",
    "        self.fc1 = nn.Linear(512,11)\n",
    "        # grapheme_root\n",
    "        self.fc2 = nn.Linear(512,168)\n",
    "        # consonant_diacritic\n",
    "        self.fc3 = nn.Linear(512,7)\n",
    "    def forward(self,x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x1 = self.fc1(x)\n",
    "        x2 = self.fc2(x)\n",
    "        x3 = self.fc3(x)\n",
    "        return({'vowel_diacritic':x1,\n",
    "                'grapheme_root':x2,\n",
    "                'consonant_diacritic':x3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from catalyst.utils import set_global_seed\n",
    "from catalyst.dl.runner import SupervisedRunner\n",
    "from catalyst.dl.callbacks import CriterionCallback, CriterionAggregatorCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_global_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = collections.OrderedDict()\n",
    "loaders[\"train\"] = train_loader\n",
    "loaders[\"valid\"] = val_loader\n",
    "runner = SupervisedRunner(input_key='image',\n",
    "                          input_target_key=None, \n",
    "                          output_key=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=3e-4, \n",
    "    weight_decay=0.001)  \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    factor=0.1, \n",
    "    patience=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterions_dict = {'vowel_diacritic_loss':torch.nn.CrossEntropyLoss(), \n",
    "                   'grapheme_root_loss':torch.nn.CrossEntropyLoss(),\n",
    "                   'consonant_diacritic_loss':torch.nn.CrossEntropyLoss(),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks=[\n",
    "    CriterionCallback(input_key='grapheme_root',\n",
    "                      output_key='grapheme_root',\n",
    "                      prefix='grapheme_root_loss',\n",
    "                      criterion_key='grapheme_root_loss', multiplier=2.0),\n",
    "    CriterionCallback(input_key='vowel_diacritic',\n",
    "                      output_key='vowel_diacritic',\n",
    "                      prefix='vowel_diacritic_loss',\n",
    "                      criterion_key='vowel_diacritic_loss', \n",
    "                      multiplier=1.0),\n",
    "    CriterionCallback(input_key='consonant_diacritic',\n",
    "                      output_key='consonant_diacritic',\n",
    "                      prefix='consonant_diacritic_loss',\n",
    "                      criterion_key='consonant_diacritic_loss', \n",
    "                      multiplier=1.0),\n",
    "    CriterionAggregatorCallback(prefix='loss',\n",
    "                                loss_keys=['grapheme_root_loss',\n",
    "                                           'vowel_diacritic_loss',\n",
    "                                           'consonant_diacritic_loss']),\n",
    "    TaskMetricCallback()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_checkpoint();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/20 * Epoch (train): 100% 2823/2823 [05:09<00:00,  9.13it/s, consonant_diacritic_loss=0.045, grapheme_root_loss=1.230, loss=1.454, vowel_diacritic_loss=0.179] \n",
      "1/20 * Epoch (valid): 100% 316/316 [00:12<00:00, 24.53it/s, consonant_diacritic_loss=0.149, grapheme_root_loss=28.856, loss=37.634, vowel_diacritic_loss=8.628]\n",
      "[2020-02-10 23:47:40,008] \n",
      "1/20 * Epoch 1 (train): _base/lr=0.0003 | _base/momentum=0.9000 | _timers/_fps=2360.2960 | _timers/batch_time=0.0073 | _timers/data_time=0.0011 | _timers/model_time=0.0061 | consonant_diacritic=0.5543 | consonant_diacritic_loss=0.4806 | grapheme_root=0.2467 | grapheme_root_loss=5.1971 | loss=6.3092 | taskmetric=0.4254 | vowel_diacritic=0.6540 | vowel_diacritic_loss=0.6314\n",
      "1/20 * Epoch 1 (valid): _base/lr=0.0003 | _base/momentum=0.9000 | _timers/_fps=3130.8237 | _timers/batch_time=0.0060 | _timers/data_time=0.0014 | _timers/model_time=0.0045 | consonant_diacritic=0.1394 | consonant_diacritic_loss=3.0568 | grapheme_root=0.0058 | grapheme_root_loss=20.2389 | loss=29.0098 | taskmetric=0.0597 | vowel_diacritic=0.0879 | vowel_diacritic_loss=5.7141\n",
      "2/20 * Epoch (train):  10% 282/2823 [00:32<05:01,  8.41it/s, consonant_diacritic_loss=0.154, grapheme_root_loss=1.983, loss=2.356, vowel_diacritic_loss=0.220]"
     ]
    }
   ],
   "source": [
    "runner.train(\n",
    "    model=model,\n",
    "    main_metric='loss',\n",
    "    minimize_metric=True,\n",
    "    criterion=criterions_dict,\n",
    "    optimizer=optimizer,\n",
    "    callbacks=callbacks,\n",
    "    loaders=loaders,\n",
    "    logdir='./test_1',\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=20,\n",
    "    verbose=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

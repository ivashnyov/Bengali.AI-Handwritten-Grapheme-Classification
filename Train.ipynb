{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFm4otoi4sbM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "outputId": "f9cab868-8dc2-4096-a936-dd2dbd9fdcbe"
      },
      "source": [
        "!pip install catalyst==20.01"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catalyst==20.01 in /usr/local/lib/python3.6/dist-packages (20.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (1.4.0)\n",
            "Requirement already satisfied: pandas>=0.22 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (0.25.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (0.10.0)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (4.42.1)\n",
            "Requirement already satisfied: plotly>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (4.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (3.1.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (20.1)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (0.5.0)\n",
            "Requirement already satisfied: GitPython>=2.1.11 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (3.0.7)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (3.13)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (2.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (4.1.2.30)\n",
            "Requirement already satisfied: crc32c>=1.7 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (2.0)\n",
            "Requirement already satisfied: safitty>=1.2.3 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (1.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (2.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (1.17.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (0.22.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (5.5.0)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (0.16.2)\n",
            "Requirement already satisfied: tensorboard>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.01) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22->catalyst==20.01) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22->catalyst==20.01) (2018.9)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from seaborn->catalyst==20.01) (1.4.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly>=4.1.0->catalyst==20.01) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly>=4.1.0->catalyst==20.01) (1.12.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst==20.01) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst==20.01) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst==20.01) (1.1.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.2.1->catalyst==20.01) (6.2.2)\n",
            "Requirement already satisfied: gitdb2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from GitPython>=2.1.11->catalyst==20.01) (2.0.6)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->catalyst==20.01) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->catalyst==20.01) (0.14.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.01) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.01) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.01) (45.1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.01) (4.4.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.01) (4.3.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.01) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.01) (2.1.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.01) (4.8.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->catalyst==20.01) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->catalyst==20.01) (2.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.01) (0.16.1)\n",
            "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.01) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.01) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.01) (0.34.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.01) (3.1.1)\n",
            "Requirement already satisfied: smmap2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from gitdb2>=2.0.0->GitPython>=2.1.11->catalyst==20.01) (2.0.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->catalyst==20.01) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->catalyst==20.01) (0.1.8)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->catalyst==20.01) (0.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r3NDtBkDzce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image as PImage\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from typing import List\n",
        "import catalyst\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTInrDzA4uZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "79f7ec33-63a0-4d2b-d66c-f8c47f1af407"
      },
      "source": [
        "# TO save for later w drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfzWnKEp4sbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_FOLDER = '/content/drive/My Drive/bengali'\n",
        "TRAIN_DATA = '/content/drive/My Drive/bengali/train'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wasuemff4sbU",
        "colab_type": "code",
        "outputId": "017f9192-0745-49f5-bf35-26bf8a5de7bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "train_data = pd.read_csv(os.path.join(DATA_FOLDER, './train_data.tsv'))\n",
        "train_data[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']] = train_data[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].astype('uint8')\n",
        "train_data.drop(['test_fold'],axis=1,inplace=True)\n",
        "train_data['type'] = 'train'\n",
        "validation_data = pd.read_csv(os.path.join(DATA_FOLDER, './validation_data.tsv'))\n",
        "validation_data[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']] = validation_data[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].astype('uint8')\n",
        "validation_data['type'] = 'validation'\n",
        "all_data = pd.concat([train_data, validation_data])\n",
        "all_data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>grapheme_root</th>\n",
              "      <th>vowel_diacritic</th>\n",
              "      <th>consonant_diacritic</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Train_0</td>\n",
              "      <td>15</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Train_1</td>\n",
              "      <td>159</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Train_2</td>\n",
              "      <td>22</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Train_3</td>\n",
              "      <td>53</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Train_4</td>\n",
              "      <td>71</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  image_id  grapheme_root  vowel_diacritic  consonant_diacritic   type\n",
              "0  Train_0             15                9                    5  train\n",
              "1  Train_1            159                0                    0  train\n",
              "2  Train_2             22                3                    5  train\n",
              "3  Train_3             53                2                    2  train\n",
              "4  Train_4             71                9                    5  train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIS-PYuuCSCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_image_ids = all_data['image_id'][all_data['type']=='train'].values\n",
        "valid_image_ids = all_data['image_id'][all_data['type']!='train'].values\n",
        "train_mask = all_data['type']=='train'\n",
        "val_mask = all_data['type']!='train'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa3VQeYg4sbX",
        "colab_type": "text"
      },
      "source": [
        "Prepare dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYuHhRsG4sbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_data = []\n",
        "for i in range(4):\n",
        "  chunk = pd.read_parquet(os.path.join(TRAIN_DATA, 'train_image_data_{}.parquet'.format(i)))\n",
        "  chunk.index = chunk.image_id\n",
        "  chunk.drop(['image_id'],axis=1,inplace=True)\n",
        "  chunk.astype(np.uint8)\n",
        "  image_data.append(chunk)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcuNWXEu_wWy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "94a48295-11da-4afa-b037-8aaada73fb63"
      },
      "source": [
        "image_data = pd.concat(image_data)\n",
        "image_data.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>32292</th>\n",
              "      <th>32293</th>\n",
              "      <th>32294</th>\n",
              "      <th>32295</th>\n",
              "      <th>32296</th>\n",
              "      <th>32297</th>\n",
              "      <th>32298</th>\n",
              "      <th>32299</th>\n",
              "      <th>32300</th>\n",
              "      <th>32301</th>\n",
              "      <th>32302</th>\n",
              "      <th>32303</th>\n",
              "      <th>32304</th>\n",
              "      <th>32305</th>\n",
              "      <th>32306</th>\n",
              "      <th>32307</th>\n",
              "      <th>32308</th>\n",
              "      <th>32309</th>\n",
              "      <th>32310</th>\n",
              "      <th>32311</th>\n",
              "      <th>32312</th>\n",
              "      <th>32313</th>\n",
              "      <th>32314</th>\n",
              "      <th>32315</th>\n",
              "      <th>32316</th>\n",
              "      <th>32317</th>\n",
              "      <th>32318</th>\n",
              "      <th>32319</th>\n",
              "      <th>32320</th>\n",
              "      <th>32321</th>\n",
              "      <th>32322</th>\n",
              "      <th>32323</th>\n",
              "      <th>32324</th>\n",
              "      <th>32325</th>\n",
              "      <th>32326</th>\n",
              "      <th>32327</th>\n",
              "      <th>32328</th>\n",
              "      <th>32329</th>\n",
              "      <th>32330</th>\n",
              "      <th>32331</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>image_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Train_0</th>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>...</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train_1</th>\n",
              "      <td>251</td>\n",
              "      <td>244</td>\n",
              "      <td>238</td>\n",
              "      <td>245</td>\n",
              "      <td>248</td>\n",
              "      <td>246</td>\n",
              "      <td>246</td>\n",
              "      <td>247</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>250</td>\n",
              "      <td>250</td>\n",
              "      <td>246</td>\n",
              "      <td>249</td>\n",
              "      <td>248</td>\n",
              "      <td>250</td>\n",
              "      <td>249</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>250</td>\n",
              "      <td>249</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>...</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train_2</th>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>249</td>\n",
              "      <td>250</td>\n",
              "      <td>249</td>\n",
              "      <td>245</td>\n",
              "      <td>247</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>251</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>249</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>250</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>...</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>250</td>\n",
              "      <td>250</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>250</td>\n",
              "      <td>250</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train_3</th>\n",
              "      <td>247</td>\n",
              "      <td>247</td>\n",
              "      <td>249</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>250</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>249</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>250</td>\n",
              "      <td>249</td>\n",
              "      <td>250</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>249</td>\n",
              "      <td>250</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>...</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train_4</th>\n",
              "      <td>249</td>\n",
              "      <td>248</td>\n",
              "      <td>246</td>\n",
              "      <td>246</td>\n",
              "      <td>248</td>\n",
              "      <td>244</td>\n",
              "      <td>242</td>\n",
              "      <td>242</td>\n",
              "      <td>229</td>\n",
              "      <td>225</td>\n",
              "      <td>231</td>\n",
              "      <td>229</td>\n",
              "      <td>229</td>\n",
              "      <td>228</td>\n",
              "      <td>221</td>\n",
              "      <td>224</td>\n",
              "      <td>226</td>\n",
              "      <td>221</td>\n",
              "      <td>221</td>\n",
              "      <td>220</td>\n",
              "      <td>217</td>\n",
              "      <td>217</td>\n",
              "      <td>218</td>\n",
              "      <td>219</td>\n",
              "      <td>222</td>\n",
              "      <td>224</td>\n",
              "      <td>214</td>\n",
              "      <td>218</td>\n",
              "      <td>227</td>\n",
              "      <td>227</td>\n",
              "      <td>227</td>\n",
              "      <td>228</td>\n",
              "      <td>224</td>\n",
              "      <td>231</td>\n",
              "      <td>235</td>\n",
              "      <td>235</td>\n",
              "      <td>233</td>\n",
              "      <td>212</td>\n",
              "      <td>183</td>\n",
              "      <td>196</td>\n",
              "      <td>...</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 32332 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0    1    2    3    4  ...  32327  32328  32329  32330  32331\n",
              "image_id                           ...                                   \n",
              "Train_0   254  253  252  253  251  ...    253    253    253    253    251\n",
              "Train_1   251  244  238  245  248  ...    255    255    255    255    254\n",
              "Train_2   251  250  249  250  249  ...    253    253    253    251    249\n",
              "Train_3   247  247  249  253  253  ...    253    253    252    251    252\n",
              "Train_4   249  248  246  246  248  ...    255    255    255    255    255\n",
              "\n",
              "[5 rows x 32332 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG2gLLY84sbe",
        "colab_type": "text"
      },
      "source": [
        "Prepare Callbacks and Dataset for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q5UgZqp4sbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, \n",
        "                 df, \n",
        "                 labels,\n",
        "                 transforms=None):\n",
        "        \n",
        "        self.df = df\n",
        "        self.labels = labels\n",
        "        self.transforms = transforms\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        flattened_image = self.df.iloc[idx].values.astype(np.uint8)\n",
        "        image = np.expand_dims(flattened_image.reshape(137, 236), 2)\n",
        "        \n",
        "        grapheme_root =  self.labels['grapheme_root'].values[idx]\n",
        "        vowel_diacritic = self.labels['vowel_diacritic'].values[idx]\n",
        "        consonant_diacritic = self.labels['consonant_diacritic'].values[idx]\n",
        "        \n",
        "        if self.transforms is not None:\n",
        "            augmented = self.transforms(image=image)\n",
        "            image = augmented['image']\n",
        "        \n",
        "        image = torch.from_numpy(image.transpose((2,0,1)))\n",
        "        grapheme_root = torch.tensor(grapheme_root).long()\n",
        "        vowel_diacritic = torch.tensor(vowel_diacritic).long()\n",
        "        consonant_diacritic = torch.tensor(consonant_diacritic).long() \n",
        "        \n",
        "        output_dict  = {\n",
        "            'grapheme_root' : grapheme_root, \n",
        "            'vowel_diacritic' : vowel_diacritic, \n",
        "            'consonant_diacritic' : consonant_diacritic, \n",
        "            'image' : image\n",
        "                       }\n",
        "\n",
        "        return output_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8TsWgWj4sbi",
        "colab_type": "text"
      },
      "source": [
        "Make train and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcJpNdfj4sbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "num_workers = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pSsVVPe4sbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from albumentations.core.transforms_interface import DualTransform\n",
        "from albumentations.augmentations import functional as F\n",
        "class GridDropout(DualTransform):\n",
        "    \"\"\"\n",
        "    GridDropout, drops out rectangular regions of an image and the corresponding mask in a grid fashion.\n",
        "        Args:\n",
        "            ratio (float): the ratio of the mask holes to the unit_size (same for horizontal and vertical directions).\n",
        "                Must be between 0 and 1. Default: 0.5.\n",
        "            unit_size_min (int): minimum size of the grid unit. Must be between 2 and the image shorter edge.\n",
        "                If 'None', holes_number_x and holes_number_y are used to setup the grid. Default: `None`.\n",
        "            unit_size_max (int): maximum size of the grid unit. Must be between 2 and the image shorter edge.\n",
        "                If 'None', holes_number_x and holes_number_y are used to setup the grid. Default: `None`.\n",
        "            holes_number_x (int): the number of grid units in x direction. Must be between 1 and image width//2.\n",
        "                If 'None', grid unit width is set as image_width//10. Default: `None`.\n",
        "            holes_number_y (int): the number of grid units in y direction. Must be between 1 and image height//2.\n",
        "                If `None`, grid unit height is set equal to the grid unit width or image height, whatever is smaller.\n",
        "            shift_x (int): offsets of the grid start in x direction from (0,0) coordinate.\n",
        "                Clipped between 0 and grid unit_width - hole_width. Default: 0.\n",
        "            shift_y (int): offsets of the grid start in y direction from (0,0) coordinate.\n",
        "                Clipped between 0 and grid unit_width - hole_width. Default: 0.\n",
        "            shift_y (int): offsets of the grid start in y direction from (0,0) coordinate.\n",
        "                Clipped between 0 and grid unit height - hole_height. Default: 0.\n",
        "            random_offset (boolean): weather to offset the grid randomly between 0 and grid unit size - hole size\n",
        "                If 'True', entered shift_x, shift_y are ignored and set randomly. Default: `False`.\n",
        "            fill_value (int): value for the dropped pixels. Default = 0\n",
        "            mask_fill_value (int): value for the dropped pixels in mask.\n",
        "                If `None`, tranformation is not applied to the mask. Default: `None`.\n",
        "        Targets:\n",
        "            image, mask\n",
        "        Image types:\n",
        "            uint8, float32\n",
        "        References:\n",
        "            https://arxiv.org/abs/2001.04086\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        ratio: float = 0.5,\n",
        "        unit_size_min: int = None,\n",
        "        unit_size_max: int = None,\n",
        "        holes_number_x: int = None,\n",
        "        holes_number_y: int = None,\n",
        "        shift_x: int = 0,\n",
        "        shift_y: int = 0,\n",
        "        random_offset: bool = False,\n",
        "        fill_value: int = 0,\n",
        "        mask_fill_value: int = None,\n",
        "        always_apply: bool = False,\n",
        "        p: float = 0.5,\n",
        "    ):\n",
        "        super(GridDropout, self).__init__(always_apply, p)\n",
        "        self.ratio = ratio\n",
        "        self.unit_size_min = unit_size_min\n",
        "        self.unit_size_max = unit_size_max\n",
        "        self.holes_number_x = holes_number_x\n",
        "        self.holes_number_y = holes_number_y\n",
        "        self.shift_x = shift_x\n",
        "        self.shift_y = shift_y\n",
        "        self.random_offset = random_offset\n",
        "        self.fill_value = fill_value\n",
        "        self.mask_fill_value = mask_fill_value\n",
        "        if not 0 < self.ratio <= 1:\n",
        "            raise ValueError(\"ratio must be between 0 and 1.\")\n",
        "\n",
        "    def apply(self, image, holes=[], **params):\n",
        "        return F.cutout(image, holes, self.fill_value)\n",
        "\n",
        "    def apply_to_mask(self, image, holes=[], **params):\n",
        "        if self.mask_fill_value is None:\n",
        "            return image\n",
        "        else:\n",
        "            return F.cutout(image, holes, self.mask_fill_value)\n",
        "\n",
        "    def get_params_dependent_on_targets(self, params):\n",
        "        img = params[\"image\"]\n",
        "        height, width = img.shape[:2]\n",
        "        # set grid using unit size limits\n",
        "        if self.unit_size_min and self.unit_size_max:\n",
        "            if not 2 <= self.unit_size_min <= self.unit_size_max:\n",
        "                raise ValueError(\"Max unit size should be >= min size, both at least 2 pixels.\")\n",
        "            if self.unit_size_max > min(height, width):\n",
        "                raise ValueError(\"Grid size limits must be within the shortest image edge.\")\n",
        "            unit_width = random.randint(self.unit_size_min, self.unit_size_max + 1)\n",
        "            unit_height = unit_width\n",
        "        else:\n",
        "            # set grid using holes numbers\n",
        "            if self.holes_number_x is None:\n",
        "                unit_width = max(2, width // 10)\n",
        "            else:\n",
        "                if not 1 <= self.holes_number_x <= width // 2:\n",
        "                    raise ValueError(\"The hole_number_x must be between 1 and image width//2.\")\n",
        "                unit_width = width // self.holes_number_x\n",
        "            if self.holes_number_y is None:\n",
        "                unit_height = max(min(unit_width, height), 2)\n",
        "            else:\n",
        "                if not 1 <= self.holes_number_y <= height // 2:\n",
        "                    raise ValueError(\"The hole_number_y must be between 1 and image height//2.\")\n",
        "                unit_height = height // self.holes_number_y\n",
        "\n",
        "        hole_width = int(unit_width * self.ratio)\n",
        "        hole_height = int(unit_height * self.ratio)\n",
        "        # min 1 pixel and max unit length - 1\n",
        "        hole_width = min(max(hole_width, 1), unit_width - 1)\n",
        "        hole_height = min(max(hole_height, 1), unit_height - 1)\n",
        "        # set offset of the grid\n",
        "        if self.shift_x is None:\n",
        "            shift_x = 0\n",
        "        else:\n",
        "            shift_x = min(max(0, self.shift_x), unit_width - hole_width)\n",
        "        if self.shift_y is None:\n",
        "            shift_y = 0\n",
        "        else:\n",
        "            shift_y = min(max(0, self.shift_y), unit_height - hole_height)\n",
        "        if self.random_offset:\n",
        "            shift_x = random.randint(0, unit_width - hole_width)\n",
        "            shift_y = random.randint(0, unit_height - hole_height)\n",
        "        holes = []\n",
        "        for i in range(width // unit_width + 1):\n",
        "            for j in range(height // unit_height + 1):\n",
        "                x1 = min(shift_x + unit_width * i, width)\n",
        "                y1 = min(shift_y + unit_height * j, height)\n",
        "                x2 = min(x1 + hole_width, width)\n",
        "                y2 = min(y1 + hole_height, height)\n",
        "                holes.append((x1, y1, x2, y2))\n",
        "\n",
        "        return {\"holes\": holes}\n",
        "\n",
        "    @property\n",
        "    def targets_as_params(self):\n",
        "        return [\"image\"]\n",
        "\n",
        "    def get_transform_init_args_names(self):\n",
        "        return (\n",
        "            \"ratio\",\n",
        "            \"unit_size_min\",\n",
        "            \"unit_size_max\",\n",
        "            \"holes_number_x\",\n",
        "            \"holes_number_y\",\n",
        "            \"shift_x\",\n",
        "            \"shift_y\",\n",
        "            \"mask_fill_value\",\n",
        "            \"random_offset\",\n",
        "        )\n",
        "    \n",
        "class AugMix(DualTransform):\n",
        "    \"\"\"Augmentations mix to Improve Robustness and Uncertainty.\n",
        "\n",
        "    Args:\n",
        "        image (np.ndarray): Raw input image of shape (h, w, c)\n",
        "        severity (int): Severity of underlying augmentation operators.\n",
        "        width (int): Width of augmentation chain\n",
        "        depth (int): Depth of augmentation chain. -1 enables stochastic depth uniformly\n",
        "          from [1, 3]\n",
        "        alpha (float): Probability coefficient for Beta and Dirichlet distributions.\n",
        "        augmentations (list of augmentations): Augmentations that need to mix and perform.\n",
        "\n",
        "    Target:\n",
        "        image\n",
        "\n",
        "    Image types:\n",
        "        uint8, float32\n",
        "\n",
        "    Returns:\n",
        "        mixed: Augmented and mixed image.\n",
        "      \n",
        "    Reference:\n",
        "    |   https://arxiv.org/abs/1912.02781\n",
        "    |   https://github.com/google-research/augmix\n",
        "    \"\"\"\n",
        "    def __init__(self, width=4, \n",
        "                 depth=3, \n",
        "                 alpha=0.5,\n",
        "                 augmentations=None,\n",
        "                 mean=[0.485, 0.456, 0.406],\n",
        "                 std=[0.229, 0.224, 0.225], \n",
        "                 always_apply=False,\n",
        "                 resize_width = None,\n",
        "                 resize_height = None,\n",
        "                 p=0.5):\n",
        "        super(AugMix, self).__init__(always_apply, p)\n",
        "        if isinstance(augmentations, (list, tuple)):\n",
        "            self.augmentations = augmentations\n",
        "        else:\n",
        "            raise ValueError(\"Augmentations list should be passed to 'augmentations' argument.\")\n",
        "        self.width = width\n",
        "        self.depth = depth\n",
        "        self.alpha = alpha\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.resize_width = resize_width\n",
        "        self.resize_height = resize_height\n",
        "\n",
        "    def apply_op(self, image, op):\n",
        "        image = np.clip(image * 255., 0, 255).astype(np.uint8)\\\n",
        "              if 'float32' not in op.__doc__\\\n",
        "              else image\n",
        "        image = op(image=image)['image']\n",
        "        return image\n",
        "\n",
        "    def apply(self, img, **params):\n",
        "        ws = np.float32(np.random.dirichlet([self.alpha] * self.width))\n",
        "        m = np.float32(np.random.beta(self.alpha, self.alpha))\n",
        "\n",
        "        mix = np.float32(np.zeros_like(img))\n",
        "        for i in range(self.width):\n",
        "            image_aug = img.copy()\n",
        "\n",
        "            for _ in range(self.depth):\n",
        "                op = np.random.choice(self.augmentations)\n",
        "                image_aug = self.apply_op(img, op)\n",
        "\n",
        "        # Preprocessing commutes since all coefficients are convex\n",
        "        mix = np.add(mix, ws[i] * F.normalize(image_aug, mean=self.mean, std=self.std), out=mix, casting=\"unsafe\")\n",
        "        mixed = (1 - m) * F.normalize(img, mean=self.mean, std=self.std) + m * mix\n",
        "        if self.resize_height is not None and self.resize_width is not None:\n",
        "            mixed = F.resize(mixed, height = self.resize_height, width = self.resize_width)\n",
        "        return mixed\n",
        " \n",
        "    def get_transform_init_args_names(self):\n",
        "        return ('width', 'depth', 'alpha', 'mean', 'std', 'height', 'width')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_UVIyaB8vZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from albumentations import (\n",
        "    HorizontalFlip, ShiftScaleRotate,\n",
        "    GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, Cutout\n",
        ")\n",
        "import albumentations as A\n",
        "\n",
        "augs = [HorizontalFlip(always_apply=True),\n",
        "        MotionBlur(always_apply=True),\n",
        "        ShiftScaleRotate(always_apply=True),\n",
        "        GaussNoise(always_apply=True),\n",
        "        MedianBlur(always_apply=True),\n",
        "        Cutout(always_apply=True),\n",
        "        GridDropout(always_apply=True)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlG8mXSG84mn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transforms_train = A.Compose([\n",
        "    AugMix(width=3, \n",
        "           depth=8,\n",
        "           alpha=.25, \n",
        "           p=1., \n",
        "           augmentations=augs, \n",
        "           mean=[0.5], \n",
        "           std=[0.5],\n",
        "           resize_height = 64,\n",
        "           resize_width = 128),\n",
        "])\n",
        "transforms_val = A.Compose([\n",
        "    A.Resize(width = 128, \n",
        "             height = 64),\n",
        "    A.Normalize(mean=(0.5), \n",
        "                std=(0.5))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9hpKtHF4sbp",
        "colab_type": "text"
      },
      "source": [
        "Callbacks for catalyst"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVqDnGVr4sbq",
        "colab_type": "code",
        "outputId": "1a401ae5-8c04-4489-f9c8-2e797d3ce964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.metrics import recall_score\n",
        "from catalyst.dl import Callback, RunnerState, MetricCallback, CallbackOrder, CriterionCallback\n",
        "\n",
        "class TaskMetricCallback(Callback):\n",
        "    '''\n",
        "    Proposed metrics:\n",
        "    import numpy as np\n",
        "    import sklearn.metrics\n",
        "\n",
        "    scores = []\n",
        "    for component in ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic']:\n",
        "        y_true_subset = solution[solution[component] == component]['target'].values\n",
        "        y_pred_subset = submission[submission[component] == component]['target'].values\n",
        "        scores.append(sklearn.metrics.recall_score(\n",
        "            y_true_subset, y_pred_subset, average='macro'))\n",
        "    final_score = np.average(scores, weights=[2,1,1])\n",
        "    '''\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        input_key: str = ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic'], \n",
        "        output_key: str = ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic'],\n",
        "        class_names: str = ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic'],\n",
        "        prefix: str = \"taskmetric\", \n",
        "        ignore_index=None\n",
        "    ):\n",
        "        super().__init__(CallbackOrder.Metric)\n",
        "        self.metric_fn = lambda outputs, targets: recall_score(targets, outputs, average=\"macro\")\n",
        "        self.prefix = prefix\n",
        "        self.output_key = output_key\n",
        "        self.input_key = input_key\n",
        "        self.class_names = class_names\n",
        "        self.outputs = [[] for i in range(3)]\n",
        "        self.targets = [[] for i in range(3)]\n",
        "\n",
        "    def on_batch_end(self, state: RunnerState):\n",
        "        \n",
        "        for i in range(3):\n",
        "            outputs = state.output[self.output_key[i]].detach().cpu().numpy()\n",
        "            targets = state.input[self.input_key[i]].detach().cpu().numpy()\n",
        "            #num_classes = outputs.shape[1]\n",
        "            outputs = np.argmax(outputs, axis=1)\n",
        "            #outputs = [np.eye(num_classes)[y] for y in outputs]\n",
        "            #targets = [np.eye(num_classes)[y] for y in targets]\n",
        "            self.outputs[i].extend(outputs)\n",
        "            self.targets[i].extend(targets)\n",
        "\n",
        "    def on_loader_start(self, state):\n",
        "        self.outputs = [[] for i in range(3)]\n",
        "        self.targets = [[] for i in range(3)]\n",
        "\n",
        "    def on_loader_end(self, state):\n",
        "        metric_name = self.prefix\n",
        "        score_vec = []\n",
        "        for i in range(3):\n",
        "            targets = np.array(self.targets[i])\n",
        "            outputs = np.array(self.outputs[i])\n",
        "            metric = self.metric_fn(outputs, targets)\n",
        "            score_vec.append(metric)\n",
        "            state.metrics.epoch_values[state.loader_name][self.class_names[i]] = float(metric)\n",
        "            \n",
        "            \n",
        "        state.metrics.epoch_values[state.loader_name][metric_name] = np.average(score_vec, weights=[2,1,1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alchemy not available, to install alchemy, run `pip install alchemy-catalyst`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8sN8okT4sbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,stride=1,kernel_size=3,padding=1,bias=False):\n",
        "        super(ResidualBlock,self).__init__()\n",
        "        self.cnn1 =nn.Sequential(\n",
        "            nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding,bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        self.cnn2 = nn.Sequential(\n",
        "            nn.Conv2d(out_channels,out_channels,kernel_size,1,padding,bias=False),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride,bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.shortcut = nn.Sequential()\n",
        "    def forward(self,x):\n",
        "        residual = x\n",
        "        x = self.cnn1(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += self.shortcut(residual)\n",
        "        x = nn.ReLU(True)(x)\n",
        "        return x\n",
        "class ResNet18(nn.Module):    \n",
        "    def __init__(self):\n",
        "        super(ResNet18,self).__init__()\n",
        "        \n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(1,64,kernel_size=2,stride=2,padding=3,bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        \n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.MaxPool2d(1,1),\n",
        "            ResidualBlock(64,64),\n",
        "            ResidualBlock(64,64,2)\n",
        "        )\n",
        "        \n",
        "        self.block3 = nn.Sequential(\n",
        "            ResidualBlock(64,128),\n",
        "            ResidualBlock(128,128,2)\n",
        "        )\n",
        "        self.block4 = nn.Sequential(\n",
        "            ResidualBlock(128,256),\n",
        "            ResidualBlock(256,256,2)\n",
        "        )\n",
        "        self.block5 = nn.Sequential(\n",
        "            ResidualBlock(256,512),\n",
        "            ResidualBlock(512,512,2)\n",
        "        )\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        # vowel_diacritic\n",
        "        self.fc1 = nn.Linear(512,11)\n",
        "        # grapheme_root\n",
        "        self.fc2 = nn.Linear(512,168)\n",
        "        # consonant_diacritic\n",
        "        self.fc3 = nn.Linear(512,7)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.block5(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x1 = self.fc1(x)\n",
        "        x2 = self.fc2(x)\n",
        "        x3 = self.fc3(x)\n",
        "        return({'vowel_diacritic':x1,\n",
        "                'grapheme_root':x2,\n",
        "                'consonant_diacritic':x3})\n",
        "class ResNet34(nn.Module):    \n",
        "    def __init__(self):\n",
        "        super(ResNet34,self).__init__()\n",
        "        \n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(1,64,kernel_size=2,stride=2,padding=3,bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        \n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.MaxPool2d(1,1),\n",
        "            ResidualBlock(64,64),\n",
        "            ResidualBlock(64,64,2)\n",
        "        )\n",
        "        \n",
        "        self.block3 = nn.Sequential(\n",
        "            ResidualBlock(64,128),\n",
        "            ResidualBlock(128,128,2)\n",
        "        )\n",
        "        \n",
        "        self.block4 = nn.Sequential(\n",
        "            ResidualBlock(128,256),\n",
        "            ResidualBlock(256,256,2)\n",
        "        )\n",
        "        self.block5 = nn.Sequential(\n",
        "            ResidualBlock(256,512),\n",
        "            ResidualBlock(512,512,2)\n",
        "        )\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        # vowel_diacritic\n",
        "        self.fc1 = nn.Linear(512,11)\n",
        "        # grapheme_root\n",
        "        self.fc2 = nn.Linear(512,168)\n",
        "        # consonant_diacritic\n",
        "        self.fc3 = nn.Linear(512,7)\n",
        "    def forward(self,x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.block5(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x1 = self.fc1(x)\n",
        "        x2 = self.fc2(x)\n",
        "        x3 = self.fc3(x)\n",
        "        return({'vowel_diacritic':x1,\n",
        "                'grapheme_root':x2,\n",
        "                'consonant_diacritic':x3})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QPEKJ9H4sbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "from catalyst.utils import set_global_seed\n",
        "from catalyst.dl.runner import SupervisedRunner\n",
        "from catalyst.dl.callbacks import CriterionCallback, CriterionAggregatorCallback, EarlyStoppingCallback"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z66ttwht4sbx",
        "colab_type": "code",
        "outputId": "6a0e62cf-4b90-4718-ae99-e929e865e9ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "set_global_seed(42)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9JssHh14scA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseile_train_resnet18():\n",
        "  train_dataset = ImageDataset(df = image_data.loc[train_image_ids,:], \n",
        "                              labels = all_data.loc[train_mask, :], \n",
        "                              transforms = transforms_train\n",
        "                              )\n",
        "  val_dataset = ImageDataset(df = image_data.loc[valid_image_ids,:],\n",
        "                            labels = all_data.loc[val_mask, :], \n",
        "                            transforms = transforms_val\n",
        "                            ) \n",
        "  train_loader = DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "      shuffle=True\n",
        "  )\n",
        "  val_loader = DataLoader(\n",
        "      val_dataset,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "      shuffle=False   \n",
        "      )\n",
        "  model = ResNet18().cuda()\n",
        "  loaders = collections.OrderedDict()\n",
        "  loaders[\"train\"] = train_loader\n",
        "  loaders[\"valid\"] = val_loader\n",
        "  runner = SupervisedRunner(input_key='image',\n",
        "                            input_target_key=None, \n",
        "                            output_key=None)\n",
        "  optimizer = torch.optim.AdamW(\n",
        "      model.parameters(), \n",
        "      lr=3e-4, \n",
        "      weight_decay=0.01)  \n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "      optimizer,\n",
        "      factor=0.1, \n",
        "      patience=5) \n",
        "  criterions_dict = {'vowel_diacritic_loss':torch.nn.CrossEntropyLoss(), \n",
        "                    'grapheme_root_loss':torch.nn.CrossEntropyLoss(),\n",
        "                    'consonant_diacritic_loss':torch.nn.CrossEntropyLoss(),}\n",
        "  callbacks=[\n",
        "      CriterionCallback(input_key='grapheme_root',\n",
        "                        output_key='grapheme_root',\n",
        "                        prefix='grapheme_root_loss',\n",
        "                        criterion_key='grapheme_root_loss', multiplier=2.0),\n",
        "      CriterionCallback(input_key='vowel_diacritic',\n",
        "                        output_key='vowel_diacritic',\n",
        "                        prefix='vowel_diacritic_loss',\n",
        "                        criterion_key='vowel_diacritic_loss', \n",
        "                        multiplier=1.0),\n",
        "      CriterionCallback(input_key='consonant_diacritic',\n",
        "                        output_key='consonant_diacritic',\n",
        "                        prefix='consonant_diacritic_loss',\n",
        "                        criterion_key='consonant_diacritic_loss', \n",
        "                        multiplier=1.0),\n",
        "      CriterionAggregatorCallback(prefix='loss',\n",
        "                                  loss_keys=['grapheme_root_loss',\n",
        "                                            'vowel_diacritic_loss',\n",
        "                                            'consonant_diacritic_loss']),\n",
        "      TaskMetricCallback(), \n",
        "      EarlyStoppingCallback(patience = 7)] \n",
        "  runner.train(\n",
        "      model=model,\n",
        "      main_metric='loss',\n",
        "      minimize_metric=True,\n",
        "      criterion=criterions_dict,\n",
        "      optimizer=optimizer,\n",
        "      callbacks=callbacks,\n",
        "      loaders=loaders,\n",
        "      logdir=os.path.join(DATA_FOLDER, './baseline_resnet18_validation_train_split'),\n",
        "      scheduler=scheduler,\n",
        "      num_epochs=50,\n",
        "      verbose=True)      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE52caZuRiZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "baseile_train_resnet18()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Yp6Q3oHFSzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}